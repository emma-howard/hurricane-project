# -*- coding: utf-8 -*-
"""Hurricane_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s81HNadUcPwyfbrQvx5VuA7oEDV0VD7w
"""

!pip install tensorflow-gpu==2.0.0rc numpy pandas folium --no-cache-dir

import tensorflow as tf
import numpy as np
import pandas as pd
import folium
import datetime as dt
import dateutil.parser
from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor
import matplotlib.pyplot as plt
import random
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
tf.__version__

#------------------------------------------------------------------------------#
#                               Data Processing                                #



# This is an object class to store all the data about storms :) 
class storm(object):

  def __init__(self,name,sid,lat,longi,basin,sub_basin,times,wind,pres): 
    # Note: the lat, long, times, wind, and pres are all arrays across the duration of a storm! 
    self.sid = sid # unique ID number associated with year/number of storm 
    self.name = name # Hurricane Name
    self.lat = lat # Hurricane Latitude, array of all 
    self.longi = longi # Hurricane Longitude, array of all 
    self.basin = basin # Hurricane basin, by 2-lettter code 
    self.sub_basin = sub_basin
    self.times = times #Hurricane times 
    self.wind = wind 
    self.pres = pres

  def Get_Maxs(this):
    maxLat = max(this.lat)
    maxLong = max(this.longi)
    maxWind = max(this.wind)
    maxPres = max(this.pres)
    return maxLat, maxLong, maxWind, maxPres

  def Get_Mins(this):
    minLat = min(this.lat)
    minLong = min(this.longi)
    minWind = min(this.wind)
    minPres = min(this.pres)
    return minLat, minLong, minWind, minPres


  def CleanStorm(this,interval):
    toRemove = []
    i = 1
    lastCleanTime = this.times[0]
    # check if the last correct time minus the current time is in the correct interval
    while(i < len(this.times)):
      nextTime = this.times[i]
      tDelta = nextTime - lastCleanTime
      if tDelta == interval:
        lastCleanTime = nextTime
      else:
        toRemove.append(i)
      i += 1

    # reverse sort to not mess up the indices 
    toRemove.sort(reverse=True)

    # Remove the bad data
    for r in toRemove:
      this.times.pop(r)
      this.longi.pop(r)
      this.lat.pop(r)
      this.wind.pop(r)
      this.pres.pop(r)      

# This function sorts imported data into storms :) 
def stormify(dataset):
  storm_list = []
  item = dataset[0] 

  stormno = item[2]
  name = item[5] 
  sid = str(item[1]*1000 + item[2])
  lat = []
  longi = []
  basin = str.strip(item[3])
  sub_basin = str.strip(item[4])
  times = []
  wind = []
  pres = []
  for item in dataset: 
    # Case 1 - New storm 
    # Create an object for the old storm, re-initialize everything 
    if (item[2] != stormno):
      """    print('Name', name)
      print('SID', sid)
      print('Basin', basin)
      print('Lat', lat)
      print('Long', longi)
      print('Times', times)
      print('Wind', wind)
      print('Pres', pres) """
      # Create an object for the given SID 
      storm_list.append(storm(name, sid, lat, longi, basin,sub_basin, times, wind, pres))
      # Reset Values with new data
      stormno = item[2]
      name = item[5] 
      sid = str(item[1]*1000 + item[2])
      lat = [item[8]]
      longi = [item[9]]
      basin = str.strip(item[3])
      sub_basin = str.strip(item[4])
      times = [dateutil.parser.parse(item[6])]
      wind = [item[10]]
      pres = [item[11]]
    # Case 2 - Same old storm 
    else:
      lat.append(item[8])
      longi.append(item[9])
      times.append(dateutil.parser.parse(item[6]))
      wind.append(item[10])
      pres.append(item[11])

  return storm_list

def normalize(x, mx, mn):
  return (x-mn)/(mx-mn)

def normalize_col(col,maxV,minV):
  for i in range(len(col)):
    col[i] = normalize(col[i],maxV,minV)
  return col

def normalize_storms(storms):
  maxLats = []
  maxLongs = []
  maxWinds = []
  maxPress = []

  minLats = []
  minLongs = []
  minWinds = []
  minPress = []

  for s in storms:
    maLa , maLo , maWi, maPr = s.Get_Maxs()
    maxLats.append(maLa)
    maxLongs.append(maLo)
    maxWinds.append(maWi)
    maxPress.append(maPr)

    miLa, miLo, miWi, miPr = s.Get_Mins()
    minLats.append(miLa)
    minLongs.append(miLo)
    minWinds.append(miWi)
    minPress.append(miPr)

  maxs = [max(maxLats),max(maxLongs),max(maxWinds),max(maxPress)]
  mins = [min(minLats),min(minLongs),min(minWinds),min(minPress)]

  for s in storms:
    s.lat = normalize_col(s.lat,maxs[0],mins[0])
    s.longi = normalize_col(s.longi,maxs[1],mins[1])
    s.wind = normalize_col(s.wind,maxs[2],mins[2])
    s.pres = normalize_col(s.pres,maxs[3],mins[3])

  return storms, maxs, mins

def remove_small_storms(storm_list,cutoff):
  refined_storms = []
  for storm in storm_list:
    if len(storm.longi) >= cutoff:
      refined_storms.append(storm)
  return refined_storms 

def get_orgnaised_storm(storm,size):
  #basin_codes = ["","SE","SI","SP","EP","WP","NA","NI","SA"]
  #sub_basin_codes = ["","MM","WA","EA","CP","NA","GM","CS","BB","AS"]
  datas = []
  labels_lat = []
  labels_long = []
  labels_wind = []
  labels_pres = []

  #basin = basin_codes.index(storm.basin)
  #sub_basin = sub_basin_codes.index(storm.sub_basin)

  lats = storm.lat
  longs = storm.longi

  winds = storm.wind
  press = storm.pres
  
  length = len(winds)
  for i in range((length+1)-size):
    new_data = []
    for j in range(size-1):
      index = j + i
   

      new_data.append(lats[index])
      new_data.append(longs[index])

      new_data.append(winds[index])
      new_data.append(press[index])

    label_index = index + 1

    datas.append(new_data)
    labels_lat.append(lats[label_index])
    labels_long.append(longs[label_index])
    labels_wind.append(winds[label_index])
    labels_pres.append(press[label_index])

  return datas, labels_lat, labels_long, labels_wind, labels_pres

def create_organised_data(storm_list, stormSize):
  data_x      = [] # lat_t0|long_t0|wnd_t0|pres_t0|lat_t1|long_t1|wnd_t1|pres_t1|...
  data_y_lat  = [] # lat at next interval 
  data_y_long = []
  data_y_wind = []
  data_y_pres = []

  for storm in storm_list:
    x, yLat, yLong, yWind, yPres = get_orgnaised_storm(storm,stormSize)
    data_x = data_x + x
    data_y_lat = data_y_lat + yLat
    data_y_long = data_y_long + yLong
    data_y_wind = data_y_wind + yWind
    data_y_pres = data_y_pres + yPres

  return np.array(data_x), np.array(data_y_lat), np.array(data_y_long), np.array(data_y_wind), np.array(data_y_pres)

def get_48h_of_storms(storm):
  input_size = 4
  output_size = 4

  datas = []
  labels = []

  lats = storm.lat
  longs = storm.longi

  winds = storm.wind
  press = storm.pres
  
  length = len(winds)
  for i in range((length+1)-input_size-output_size):
    new_data = []
    new_label = []
    for j in range(i,i+input_size):
      index = j

      new_data.append(lats[index])
      new_data.append(longs[index])

      new_data.append(winds[index])
      new_data.append(press[index])

    label_index = index

    for k in range(output_size):
      label_index = label_index + 1
      sub_label = []
      
      sub_label.append(lats[label_index])
      sub_label.append(longs[label_index])

      sub_label.append(winds[label_index])
      sub_label.append(press[label_index])
      
      new_label.append(sub_label)

    datas.append(new_data)
    labels.append(new_label)

  return datas, labels

def create_48h_of_storm_data(storms):
  data_x = []
  data_y = []

  for storm in storms:
    xs , ys = get_48h_of_storms(storm)
    data_x = data_x + xs
    data_y = data_y + ys

  return data_x, data_y

#------------------------------------------------------------------------------#






#------------------------------------------------------------------------------#
#                                   Models                                     #
class Meta_Model:
  def __init__(self,lat_model, long_model, wind_model, pres_model):
    self.lat_model = lat_model
    self.long_model = long_model
    self.wind_model = wind_model
    self.pres_model = pres_model

  def predict24h(self,data):
    preds = []
    for i in range(4):
      pred = self.predict6h(data)
      preds.append(pred)
      data = np.append(data[4:],pred)
    return preds

  def predict6h(self,data):
    lat = self.lat_model.Predict(data)
    longi = self.long_model.Predict(data)
    wind = self.wind_model.Predict(data)
    pres = self.pres_model.Predict(data)
    return [lat,longi,wind,pres]

  def unNormalize(self,data):
    lat = self.lat_model.unNormalize(data[0])
    longi = self.long_model.unNormalize(data[1])
    wind = self.wind_model.unNormalize(data[2])
    pres = self.pres_model.unNormalize(data[3])
    return [lat,longi,wind,pres]

  def EvaluateModel(self, data, labels):
    num = len(data)

    mse6h = 0
    mae6h = 0

    mse12h = 0
    mae12h = 0

    mse18h = 0
    mae18h = 0

    mse24h = 0
    mae24h = 0

    mseAll = 0
    maeAll = 0

    i6h = 0
    i12h = 1
    i18h = 2
    i24h = 3
    i = 1
    for x , y in zip(data,labels):
      pred = self.predict24h(x)
      print(i,"/",len(data))
      i = i+1
      
      mse6h += mean_squared_error(y[i6h],pred[i6h])
      mae6h += mean_absolute_error(y[i6h],pred[i6h])

      mse12h += mean_squared_error(y[i12h],pred[i12h])
      mae12h += mean_absolute_error(y[i12h],pred[i12h])

      mse18h += mean_squared_error(y[i18h],pred[i18h])
      mae18h += mean_absolute_error(y[i18h],pred[i18h])

      mse24h += mean_squared_error(y[i24h],pred[i24h])
      mae24h += mean_absolute_error(y[i24h],pred[i24h])

      mseAll += mean_squared_error(y,pred)
      maeAll += mean_absolute_error(y,pred)

    mse6h = mse6h / num
    mae6h = mae6h / num

    mse12h = mse12h / num
    mae12h = mae12h / num

    mse18h = mse18h / num
    mae18h = mae18h / num

    mse24h = mse24h / num
    mae24h = mae24h / num

    mseAll = mseAll / num
    maeAll = maeAll / num

    print("--------------------")
    print("6h MSE:",mse6h)
    print("6h MAE:",mae6h)
    print("")
    print("12h MSE:",mse12h)
    print("12h MAE:",mae12h)
    print("")
    print("18h MSE:",mse18h)
    print("18h MAE:",mae18h)
    print("")
    print("24h MSE:",mse24h)
    print("24h MAE:",mae24h)
    print("")
    print("Overall MSE:",mseAll)
    print("Overall MAE:",maeAll)
    print("--------------------")

  def __reshapeInput(self,data):
    new_data = [[],[],[],[]]
    new_data[0] = data[:4]
    new_data[1] = data[4:8]
    new_data[2] = data[8:12]
    new_data[3] = data[8:]
    return new_data
  
  def __getPoints(self,data):
    points = []
    for x in data:
      lat = x[0]
      lon = x[1]
      p=(lat,lon)
      points.append(p)
    return points

  def Visualize(self,data,label):
    pred = self.Predict24h(data,label)
    data = self.__reshapeInput(data)
    
    # unNormalize all data
    for i in range(len(pred)):
      label[i] = self.unNormalize(label[i])
      pred[i] = self.unNormalize(pred[i])
      data[i] = self.unNormalize(data[i])
    
    orig_path = self.__getPoints(data)
    pred_path = self.__getPoints(pred)
    actu_path = self.__getPoints(label)

    ave_lat = sum(p[0] for p in orig_path)/len(orig_path)
    ave_lon = sum(p[1] for p in orig_path)/len(orig_path)

    my_map = folium.Map(location=[ave_lat, ave_lon], zoom_start=5)

    plotLine(my_map,orig_path,"black")
    plotLine(my_map,pred_path,"red")
    plotLine(my_map,actu_path,"blue")

    return my_map

class Hurricane_Model:
  def __init__(self, train_x, train_y,test_x,test_y, maxV, minV,name):
    self.maxV = maxV
    self.minV = minV 
    self.train_x = train_x
    self.train_y = train_y
    self.test_x = test_x
    self.test_y = test_y
    self.name = name
    self.input_dim = len(self.train_x[0])
    self.output_dim = 1

    print("Shell Created:",self.name)

  def unNormalize(self,d):
    orig = d * (self.maxV - self.minV) + self.minV
    return orig
  
  def CreateModel(self,layer1,layer2,opt):
    model = Sequential()
    model.add(Dense(layer1,input_dim=self.input_dim,activation="relu"))
    model.add(Dense(layer2, activation='relu')) 
    model.add(Dense(self.output_dim, activation='linear'))

    model.compile(loss='mse', optimizer=opt, metrics=['mse','mae'])
    self.model = model
    print("Model Created:",self.name)

  def TrainModel(self,split,epochs,batchsize):
    print("Training Model:",self.name)
    randomize = np.arange(len(self.train_x))
    np.random.shuffle(randomize)
    self.train_x = self.train_x[randomize]
    self.train_y = self.train_y[randomize]

    history = self.model.fit(self.train_x,self.train_y,epochs=epochs,batch_size=batchsize,verbose=1)
    self.__plotHistory(history)
    
  def EvaluateModel(self):
    print("Evaluating Model:",self.name)
    evalu = self.model.evaluate(x=self.test_x, y=self.test_y)
    for metric,value in zip(self.model.metrics_names,evalu):
      print(metric,":=",value)

  
  def Predict(self,X):
    dX = np.array([X])
    pY = self.model.predict(dX)[0][0]
    return pY


  def __plotHistory(self,history):
    # "Loss"
    plt.plot(history.history['loss'])
    #plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train'], loc='upper left')
    plt.show()
#------------------------------------------------------------------------------#



#------------------------------------------------------------------------------#
#                            Data Visualization                                #

def addTextToPoint(point,text):
  iframe = folium.IFrame(text, width=150, height=50)
  popup = folium.Popup(iframe, max_width=150)

  Text = folium.Marker(location=point, popup=popup,
                      icon=folium.Icon(icon_color='white'))
  return Text

def plotPoint(map,point, colour,text):
  return 0

def plotLine(my_map,path,colour):
  folium.PolyLine(path, color=colour, weight=2.5, opacity=1).add_to(my_map)
#------------------------------------------------------------------------------#

# Import the raw IBtracs hurricane data stored on GitHub
url = 'https://raw.githubusercontent.com/emma-howard/hurricane-project/master/Dataset/Allstorms.ibtracs_wmo.v03r10.csv'
cols = ['sid','year','num','basin','sub_basin','name','time','Nature','lat','long','wind','pres', "center","wind %", "pres %","Center"]
df = pd.read_csv(url, skiprows= 1, header=1)
# Coulumns - 
# Who knows | year | num in year | basin | sub_basin | name | yy-mm-dd time | Nature | lat | long | wind (wmo) | pres (wmo) | center
basin = "NA"
df["BB"] = df["BB"].apply(str.strip)
df = df.loc[df["BB"] == basin] # get only NA basin
df = df.loc[df["kt"] > 0] # Remove remove invalid data
df = df.loc[df["mb"] > 0] # Remove remove invalid data

min_year = 1980
dataset_1980p = df.loc[df["Year"] >= min_year]

raw_data = dataset_1980p.values

sorted_storms = stormify(raw_data)

interval = dt.timedelta(hours=6)
for s in sorted_storms:
  s.CleanStorm(interval)
  
storm_size = 5 # 4 previous time intervals, 1 for next iterval 
sorted_storms = remove_small_storms(sorted_storms,8) # remove storms that lassted less then 48 hours 

sorted_storms, maxs, mins = normalize_storms(sorted_storms)

split = 0.80
index = int(len(sorted_storms) * split)

trainStroms = sorted_storms[:index]
testStorms = sorted_storms[index:]

train_x, train_y_lat, train_y_long, train_y_wind, train_y_pres = create_organised_data(trainStroms,storm_size)
test_x, test_y_lat, test_y_long, test_y_wind, test_y_pres = create_organised_data(testStorms,storm_size)

print("Data Imported and ready for use")

lat_model = Hurricane_Model(train_x,train_y_lat,test_x,test_y_lat,maxs[0],mins[0],"Latitude Model")
opt = tf.keras.optimizers.Adam()
lat_model.CreateModel(256,128,opt)
lat_model.TrainModel(0.8,1000,50)

lat_model.EvaluateModel()

long_model = Hurricane_Model(train_x,train_y_long,test_x,test_y_long,maxs[1],mins[1],"Longitude Model")
opt = tf.keras.optimizers.Adam()
long_model.CreateModel(256,128,opt)
long_model.TrainModel(0.8,1000,50)

long_model.EvaluateModel()

wind = 2
wind_model = Hurricane_Model(train_x,train_y_wind,test_x,test_y_wind,maxs[wind],mins[wind],"Wind Model")
opt = tf.keras.optimizers.Adam()
wind_model.CreateModel(256,128,opt)
wind_model.TrainModel(0.8,1000,50)

wind_model.EvaluateModel()

pressure = 3
pres_model = Hurricane_Model(train_x,train_y_pres,test_x,test_y_pres,maxs[pressure],mins[pressure],"Pressure Model")
opt = tf.keras.optimizers.Adam()
pres_model.CreateModel(256,128,opt)
pres_model.TrainModel(0.8,1000,50)

pres_model.EvaluateModel()

MetaModel = Meta_Model(lat_model,long_model,wind_model,pres_model)
ind = -55
x = MetaModel.predict24h(test_x[ind])
y = [test_y_lat[ind],test_y_long[ind],test_y_wind[ind],test_y_pres[ind]]
y = MetaModel.unNormalize(y)
print(y)
for pred in x:
  print(MetaModel.unNormalize(pred))

index = int(len(sorted_storms) *0.80)
testing_48h = sorted_storms[index:]
storms24h,labels24h = create_48h_of_storm_data(testing_48h)


print(type(storms24h))
print(type(labels24h))
#MetaModel.EvaluateModel(storms24h[:750],labels24h[:750])

demo_index = random.randint(0,len(storms24h)) # get the random storm segment
demo_x = storms24h[demo_index]
demo_y = labels24h[demo_index]

show(MetaModel.Visualize(demo_x,demo_y))